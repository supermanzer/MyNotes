{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"My Notes","n":0.707},"1":{"v":"\n## Welcome to my brain\n\nI use this wiki site and [Dendron](https://wiki.dendron.so/) to organize my thoughts about various software projects and ideas.  This tool is primarily to store my own thoughts and allow me to document my development process.  This helps me better organize these projects and plan future development.  \n\nIf others find this site interesting and possibly useful, bonus!\n\n![[projects]]","n":0.13}}},{"i":2,"$":{"0":{"v":"Projects","n":1},"1":{"v":"\n---\n The root page to organize my personal projects and related notes\n \n- [[projects.ndbc-reader]]\n\n- [[projects.local-recipe-server]]\n\n- [[projects.personal-website]]","n":0.25}}},{"i":3,"$":{"0":{"v":"Personal Website","n":0.707},"1":{"v":"_A series of notes about work done on my personal website project_\n\n---\n\n**This is a work in progress so these notes may be rather haphazard**\n\n","n":0.204}}},{"i":4,"$":{"0":{"v":"Todo List","n":0.707},"1":{"v":"_A list of tasks I need to knock out for my website_\n\n* [ ] Add a blog post about data visualizations (with examples).  \n* [x] Revise welcome message to reflect redesign (\"Blog\" not \"Neat Stuff\")\n* [x] Write algorithm to determine categories of Blog posts based directories within the `blog` content directory\n* [x] Write algorithm to query all unique tags on all blog posts and return them\n* [ ] Re-watch Debbie Reynold's videos of creating a Blog using Nuxt Content focusing on implementation of the Search feature.","n":0.108}}},{"i":5,"$":{"0":{"v":"Style Issues","n":0.707},"1":{"v":"\n_Thoughts, questions, and difficulties experienced when it comes to styling my site in a cohesive manner_\n\n---\n\n## Dark/Light Mode\n\nI now have a configured a `DarkMode` switcher that hooks into the global `$vuetify.theme.isDark` property.  This allows me to toggle Dark mode on/off.  However there are some scenarios that I am running into where working with a global `dark: true|false` is proving difficult.\n\n### Nav Headers\nWhile the general light/dark mode makes sense in most situations, on pages where I have large hero images with a darkening gradient overlay I will need a way to enforce `dark: true` without actually changing this setting.  CHanging the setting would require a user to reset this value every time they navigate to a page that requires dark mode and then to another page. \n\nAfter typing this it becomes clear that the best option here would be to use a `darkMode` prop for the `NavHeader` component that defaults to `false`.  Then I would create a computed property that determines the styling applied to all text in the NavBar by the following formula: \n```\n  this.darkMode || this.$vuetify.theme.isDark\n```\n\nThen, when loading any page that uses a `layout` that requires specific styling regardless of the selected theme mode, I can simply pass the prop of `:darkMode=\"true\"`.\n\n### Custom Content\n\nAt earlier stages of development I wrote style rules directly on various components.  While this makes it easier for me to rapidly adjust styling and get a feel for how different styles look, if left in place it makes changing overall styles a huge [PITA](https://www.netlingo.com/word/pita.php#:~:text=Pain%20In%20The%20Ass,jargon%20or%20text%20message%20shorthand). So now, as the stylistic focus of my site shifts to defining cohesive global styles, my focus will be on those components with hard-coded styles and how I can use Vue's [Class and Style Bindings](https://vuejs.org/guide/essentials/class-and-style.html) to achieve some component specific styles while still adhering to a global style approach that is responsive to a user's selection of Light or Dark mode.","n":0.057}}},{"i":6,"$":{"0":{"v":"Blog Section","n":0.707},"1":{"v":"_Notes on the design and functionality of the blog portion of my personal site_\n\n---\n\n## Home Page\nI think for this section it would make the most sense to display a list view of articles sorted chronologically (most recent first) on the left and a menu of topics and tags on the right.\n\n### List View Component\nThis should be a reusable component that displays title, date created/updated, and a brief description of the article.\n\n### Nav Sidebar\nI am thinking that I will use the top level directories (within the `blog` directory) to define general categories of blog post.  These will be represented by an accordion menu. Clicking the category name text will filter the list menu on the right.  However there is a dropdown carat icon that will expand the accordion and show a list of blog post titles (sorted in reverse chronological order).  Clicking an individual title should take the user to a detail view for that article.\n\nOne aspect I'm a little undecided about is whether to retain the sidebar when showing the detail section.  It would reduce the available space and add distraction.  However it would allow faster navigation from one article to another.  A third option from either full display or no menu is to either use a [popover sidenav](https://vuetifyjs.com/en/components/navigation-drawers/#bottom-drawer). This might interfere with the sidenav component in `default` layout.  In that case I could try using a [minified design](https://vuetifyjs.com/en/components/navigation-drawers/#expand-on-hover) with `expand_on_hover`.\n\n\n\n### Algorithms and Content\nThere are two ways I can think of to allow the sidebar, both category and tag filters, to alter the content displayed in the list/detail view.  \n\n* **Store/State** This approach is to use Vuex global state.  This will entail creating a `blog` namespace, similar to `nav`, with it's own `state`, `getters`, and `mutators`.  This is a not inconsiderable addition of complexity.  However it does abstract the details of the implementation to the Vuex methods, which is appealing.\n\n* **Props/Events** This approach is the more classic Vue mechanism of propagating changes from one component to another.  THe components being acted on generate events that the parent(s) listen for.  The local state is contained on the parent and passed down as props to the display components.  In this scenario the blog home page would need to store all the state and perform the changes based on the events triggered. Navigating to a detail view would (_I think_) clear the state but that could be the behavior I want.  \n\nI was planning on having the side nav trigger a return to the list view **and** the associated filtering of blog results.  That may be prove difficult to achieve in one or the other options spelled out here.  I may need to re-evaluate this approach entirely after I get a little farther into development of the blog sidenav component.\n\n## Update\n\nCategories proved to be too much of an extra hurdle and present organizational difficulties in terms of blog topics (is my NDBC software `tech` or `science`?).  I have scrapped that in favor of wider usage of tags and the ability to toggle between an inclusive and exclusive filtering (`any` versus `all`).","n":0.044}}},{"i":7,"$":{"0":{"v":"NDBC Reader","n":0.707},"1":{"v":"\n* **Project:** \n\nThese notes are being added only after the project's complexity has reached a point where I feel the need to jot things down in order to keep track of everything in my head.  Additionally, even though these are notes to myself about my own software projects, I still write as if I'm explaining these ideas to someone else.  It's a habit I picked up when I started coding in grad school and, since it works for me, I do not feel any need to change. Alright, on to the confusing bits!\n\n---\n\n## Update -- Feb 17, 2023\n\nI worry going all in on a modular approach is introducing unnecessary complexity. I still think the separation of concerns achieved by splitting out the functionality into separate classes with well defined scopes is a net positive. To try and achieve this without unnecessary complexity I will start by documenting how I plan to organize the files in the `/src/NDBC` directory for this package.\n\n![[projects.ndbc-reader.files]]\n\n## Plan -- Sep 4, 2022\n\nThis project has been neglected since I changed jobs and have had to spend more of my free time learning different programming languages and paradigms in order to keep up.  However that comes with a bonus of exposure to more enterprise grade software architecture.  With that in mind I plan to make a more rigorous effort to use a more design pattern driven approach to the NDBC Reader package.  \n\nWhat follows is a first draft of the patterns and approaches I think would make sense to employ in this effort.\n\n* **Data Store/Data Transfer Object:** This is a more codified approach to storing the various elemets of the data that the NDBC Reader project is the focus of.  This will also encapsulate saving a Data Buoy object to disk and re-instantiating from saved data.\n* **Repository:** The repository pattern encapsulates the logic required to create, delete, and update the data.  \n* **API Services:** These services will encapsulate the logic required to make specific API requests.  \n* **Analysis Service:** This will encapsulate the rudimentary analyses that will be added to the NDBC Reader package and the Data Buoy class.\n* **Unit of Work:** This will abstract the communication between the API services (fetching data) and the repository (storing data).\n\n## Revise Package Creation using PyScaffold\nAs a way to facilitate better package organization, testing, and documentation, I want to refresh the package using [PyScallfold](https://pyscaffold.org/en/stable/index.html).  This should allow me to revise how the NDBC DataBuoy package is organized and contributed to in a much more rigorous fashion without too much effort on my part.\n\n\n### Deprecated Doc\nI had previously begun documenting how to split responsibilities into two separate classes.  After consideration I have decided this was a half-hearted attempt at modularity and I would be better served by following a more rigorous approach to design principles.  These docs have been removed.","n":0.046}}},{"i":8,"$":{"0":{"v":"Desired Functionalities","n":0.707},"1":{"v":"The [[projects.ndbc-reader]] software package has only a few operations, currently.  These mostly focus on retrieving and persisting data.  TThe other Functionalities are allowing access to the data stored during runtime and keeping track of data fetching operations performed and their outcome.\n\nI am documenting these Functionalities at a high level to help me plan and design the package in a more easily maintained, more highly abstracted manner.\n\n## Data Fetching\nThe primary purpose of the NDBC Data Buoy class is to simplify the fetching of monthly/annual summary data from the NDBC public websites.  These are exposed as simple `.txt` files hosted on an individual station's web page.  While there are other data archive formats and servers, I find this approach to be refreshingly direct and am sticking with it for now.\n\nA secondary data fetching action is to parse the data station web page and collect station metadata (instrumentation, location, etc.). \n\nBoth data fetching operations need to dynamically build the URLs fetched based on parameters specified at runtime.  They also need to track a failure to retrieve the data requested.  These features could be abstracted into a custom HTTP Client class.\n\nThe assignment of the returned data to the correct properties on the Data Buoy class could also be abstracted to a method designed for that purpose.\n\n## Data Persistence\nThis is the storing of the data that comprises the state of an instance of the Data Buoy class.  Ideally researchers should be able to use this package to retrieve data and then store it in a manner they find convenient so it can be analyzed later without requiring an internet connection.\n\nPresently the only approach to data persistence/loading is writing to / reading from `.json` files.  I think it would make sense to abstract general data persistence to it's own class and work to add multiple methods including allowing users to specify database connection parameters and store individual Data Buoy data to either SQL or NoSQL databases.\n\n## Data Storage/Access\nThe NDBC `DataBuoy` class is operates as a data storage and access API during runtime after data has been retrieved from NDBC servers.  Keeping this data organized in a way that makes sense is something that desrves serious consideration.  \n\n### Data Types:\n* **Station Metadata:** This represents information about the intrument platform itself.  While often not the subject of analyses, this information can be necessary to various research operations such as placing the station in a geographic coordinate plane, transforming wind speed and direction into wind stress, and making valid comparisons between instrument platforms.\n    * This is stored in a `station_info` property of the class\n\n* **Summary Data:** This data represents monthly/annual summary data for one of the supported data packages.  This is the primary data used for analyses.  The class must support incremental addition to this data while still providing access to the entire amount in a single interface.  An example of this is as follows:\n\nA user instantiates a buoy class and retrieves the last 3 full months of data.  The user can examine any measurement for the entire timespan covered in a single columnar vector.  If the user then wants to add the previous 12 months they should be able to do so while still being able to examine the full 15 months of data as a single data property.  Maintaining clear and consistent access to this data is crucial to the utility of this package.\n\n* **Realtime Data:** This is the most recent, non-summarized data collected by the platform.  While the measurements are the same as the Summary Data, this data has not gone through NDBC QA/QC processes and should be kept separate from summary data and clearly labeled.\n","n":0.041}}},{"i":9,"$":{"0":{"v":"Files","n":1},"1":{"v":"_Definitions for the files in the `/src/NDBC` directory comprising the functionality of the NDBC package_\n\n---\n\n* `NDBC.py` - This file contains the highest level class(es) of the package and exposes the API intended for use in retrieving and analyzing NDBC station data.\n* `models.py` - This file defines the classes specific to the domain of this software, focused on encapsulating the representation of an NDBC data station.\n* `api.py` - This file defines the classes/methods involved with making the HTTP requests to retrieve NDBC data and parsing the responses.\n* `repository.py` - This file defines the classes/methods used to save and load the state of a particular instance of an NDBC data station.  It will encapsulate different methods for different approaches to data storage such as text files (`.json`, `.csv`) or SQL/NoSQL DBs.\n* `constants.py` - This file defines all the hard-coded constant values used in the NDBC package","n":0.083}}},{"i":10,"$":{"0":{"v":"Design","n":1},"1":{"v":"_Capturing ongoing notes regarding software architecture and design of the NDBC Python package_\n\n---\nTo assist in defining the design I wish to achieve, I will be following the book [Architecture Patterns with Python](https://www.oreilly.com/library/view/architecture-patterns-with/9781492052197/).  I will skip that parts I do not feel apply but this is the reference I will be using to sketch out the design of this package in terms of commonly used design patterns and software architecture.\n\nI had worked through this book a little earlier but I always find it hard to attach meaning, and therefore retain knowledge, when the project being worked on isn't something I care about.  For this effort I will be working through the chapters in the book and applying those aspects that I think pertain to the design of the NDBC package.\n\n**No functionality should change as a result of this refactor**\n\nIdeally this approach will instead make maintenance easier and make it more plain where modifications need to be made to provide the additional functionalities I have planned for this package.","n":0.077}}},{"i":11,"$":{"0":{"v":"Repository","n":1},"1":{"v":"_Applying the Repository pattern for abstracting data storage_\n\n---\n\n![[projects.ndbc-reader.design.repository.persistence]]\n\nLet's start with a simple diagram of what we are trying to achieve here.\n\n```mermaid\nflowchart TB\n    subgraph domain\n        A[NDBC API] --> B[DataPackage]\n        B --> C[DataBuoy]\n    end\n    subgraph Repository\n        D[Abstract Repository] \n        E[Concrete Repositories]\n        E -- implements -->D\n    end\n    D --> C \n    E --> F[File System]\n    \n```\n\nThis design pattern will abstract away the data layer of our software from the model/business logic.  We are attempting to split this software (originally conceived as a single class with a large number of functions) into a layered, or more accurately onion, architecture design.  Let's make another diagram for this approach so we know where we're going.\n\n```mermaid\nflowchart TB\nA[High Level Functions]\nB[Domain Model]\nC[Data Storage/Respository]\nA --> B\nC --> B\n```\n\nThe book (architecture patterns in Python) is assuming the use of a database and using and SQLAlchemy ORM to handle interactions.  In our case we are just looking to write to a file but we can apply some of the same principles to help make our [[projects.ndbc-reader.design.domain]] unaware of how the data is loaded/saved.\n\n ## Dependency Inversion Principle\n One of the major goals of the design patterns outlined in _Architecture Patterns in Python_ is that our high-level business logic should have no direct dependencies.  Rather than our higher level software being dependant on lower level code, we seek to invert this relationship, with our lower-level, less abstract/more concrete, code being dependant on our high-level, more abstract business logic.\n\n The purpose for this is that it allows for modification to the concrete implementation of various functionalities without modifying the code in our business logic.  A good way to identify this in practice when reviewing the actual code written is which files depend or \"know about\" which others.  If we do it right in the context of this project the `NDBC.DataBuoy` class should know nothing about \n 1. How we make HTTP requests to NDBC web servers\n 1. How we save `DataBuoy` data to disk\n 1. How we retrieve data from disk.\n\n\n ### Implementation\n\n In the book, the authors are making use of SQLAlchemy ORM tools to create an ORM that depends on our domain models.  In the case of NDBC, the ORM should be responsible for mapping the `DataBuoy` contents to a specific JSON document structure.\n\nI'm not entirely sure how best to accomplish this but I'm going to start simple and refactor often.  I will start by creating a, potentially superfluous, ORM to abstract read/write operations.  \n\n**Observation:** As I work through this process I see plenty of places where I have hard-coded values.  This makes me cringe but at the same time I know I can go _way_ overboard in refactoring.  Like with SQL databases, any value that is used multiple times is often defined in a single place and referenced.  However, it is possible to go too far in this respect.  While I have seen some Python code abstract all constants to a separate file/package, I think this moves constant definitions too far from where they are used making it difficult to understand their importance.  I try to remember these two important lines from the `Zen of Python`\n\n> special cases aren't special enough to break the rules\n\n> but practicality beats purity\n\n\nTo that end I have created a `BuoyORM` class that currently handles reading from and writing to `.json` files.  I've abstracted out _most_ of the hardcoded key values to facilitate using them in both read & write operations.  This is just the precursor to properly implementation of the Repository pattern but we need to start somewhere don't we?\n\n### End Goal\nHere's a quick sketch that will help me keep track of what I want.\n```mermaid\nflowchart TB\na[Application Layer]\n\nc[Repository]\nd[Persistence]\na ----> c\nc --Domain Model Objects --> a\nc --> d\nd --> c\n```\nThe purpose of the repository layer is to abstract the complexity of how we save/load domain models.  The application layer asks for an object and receives an instantiated domain model.  It does not need to know how that gets done.","n":0.039}}},{"i":12,"$":{"0":{"v":"Persistence","n":1},"1":{"v":"At the time of this writing the current persistence method is to write out the necessary data to a `.json` file.  ","n":0.218}}},{"i":13,"$":{"0":{"v":"Domain","n":1},"1":{"v":"_Applying the domain model to the NDBC Python package_\n\n---\n\n> The _domain_ is a fancy way of saying _the problem you're trying to solve_\n\n\nThis chapter focuses on naming classes and methods in a manner that utilizes \"business jargon\" both to help the software components map to user needs as well as make deciding where to add functionality clear.\n\nThis is already partly implemented in the naming of the main class (`DataBuoy`) and the exposed method names.  This chapter also includes examples of building tests of increasing complexity to reflect the required functionalities of the software.  However, let's try to be a bit more rigorous about defining the responsibilities of the `DataBuoy` class and then evaluate whether the current testing suite does a good job of capturing all this functionality or if there is anything missing/extra.\n\n## Problem\nAccessing data from NDBC data stations is cumbersome and not easily scripted.\n\n## DataBuoy responsibilities - for now\n* Allow users to request NDBC data packages for a given station for any time period.\n    * Return data package as `pandas DataFrame` if it exists\n    * Return a warning message if it does not\n\n* Allow users to save the current state of a DataBuoy object to disk\n* Allow users to search for data stations based on geographic coordinates\n\nPresently the tests in `test_databuoy.py` appear to cover all of these aspects of the domain.  However, one aspect covered in this chapter has not been addressed.  That is whether a DataBuoy object should be treated as an Entity or a Value Object.  How should we implement equality?  Two objects could have the same `station_id` but different data packages and/or time periods.  Hmmmmmm......\n\n\n![[projects.ndbc-reader.design.domain.models]]\n\n","n":0.061}}},{"i":14,"$":{"0":{"v":"Domain Models","n":0.707},"1":{"v":"\nOne of the concepts that gets introduced in the chapter on domain is the creation of model classes to represent different objects/concepts in my code.  The original structure of the `NDBC` package contained all necessary functionality in a single `DataBuoy` class (since my focus was on gathering data from NDBC buoy stations).  \n\nHowever, this is not ideal and can make maintenance difficult.  To that end I have begun creating classes to represent identified logical/physical entities.\n\n## Models\n* `DataPackage` - This is a `dataclass` that used to encapsulate the details of an individual data package provided from an NDBC data station.  Examples are the standard meteorogical data (`stdmet`), continuous wind data (`cwind`), and others.  \n\n* `DataPackages` - This is a basic class that stores instances of `DataPackage` and will be a property of the `DataStation` class.  The primary responsibilities will be to add new or append to existing `DataPackage` instances as well as return a `DataPackage` instance when requested.\n\n* `DataStation` - This model represents the physical station where instruments are deployed and measurements are collected.\n\n* `StationInfo` - This model represents the information about a specific `DataStation`, including geographic location, instrument deployment.  \n    * **note:** - Because different data stations have different instrumentation and other pertinent information, I think this data model will need to be dynamically created using the [make_dataclass](https://docs.python.org/3/library/dataclasses.html#dataclasses.make_dataclass) function.\n","n":0.068}}},{"i":15,"$":{"0":{"v":"Local Recipe Server","n":0.577},"1":{"v":"\n## Motivation\nI love cooking and recipes.  Unfortunately I have amassed enough magazines and cookbooks that, when I want to make a particular recipe, I give up half way because finding it so laborious.  I realized I don't need hard copies, I need a searchable database with an appealing interface.  Also I've gathered more than a few Raspberry Pis and I wanted to do something cool with one (or more) of them.\n\nAlso a pet peeve of mine is how many recipe websites take up your entire phone/tablet sceen with banner adds, random stories only vaguely related to the recipe at hand, and overlapping vidoes you must carefull exit out of to avoid being redirected to some page about home loans, exciting new athletic socks, stock tips, etc.  I wanted to build an interface for viewing a recipe that was JUST THE RECIPE.\n\n## Project Definition\nThe purpose of this project is to allow users to run a server on their local (home) wireless network that does 2 things related to recipes.\n\n1. Take photographs of recipes in cookbooks or magazines and save them to the server\n1. View recipes broken down by ingredients and steps in a mobile friendly interface\n\n## [[projects.local-recipe-server.user-interfaces]]\n\nThe user interacts with this application through two distinct interfaces with separate areas of concern.\n\n* Photo upload interface [Create Recipe]\n* Recipe search/view interface [List/Retrieve Recipe]\n\n## [[projects.local-recipe-server.data-models]]\n\nA rough sketch of the data structures we will use to provide data persistence and all those nifty searching and filtering mechanisms.\n\n## [[projects.local-recipe-server.services]]\nThese are encapsulated pieces of business logic that perform the tricky custom bits of the application.\n\n## [[projects.local-recipe-server.api]]\nThe endpoints provided by the server and consumed by the client application to provide the user with CRUD capabilities for recipes.\n\n## Potential Pi Architecture\nThis will depend on the processing power and storage needs of all the above requirements.  Given the above definitions are already biting off a fair amount of development work I will likely try to keep this to a single Raspberry Pi 4B.  I'll use a 128GB flash drive for storage so that _should_ provide the capacity to store a decent amount of recipes.  While I will likely be using Docker containers to build this application (2 minimum - Client App Server, API Server), I will start with all of them running on a single Pi.  If this proves too much work I will consider tackling the whole distributed k8s mess but for now I'll try to keep it simple. \n\nWith simplicity in mind I plan to stick to just using the SQLite database.  It functions perfectly well with solid performance when accessed by a single process.  Since this application is desinged for a single user making sequential requests, this should be sufficient.\n\n## STRETCH Goal\nThis project will begin using a basic modern web site architecture using a REST API and a Vue.js front-end (my preferred framework).  This should allow for the development of an Android App that communicates with the REST API.  It's an area of development I have absolutely no experience in but it might be fun to try.\n","n":0.045}}},{"i":16,"$":{"0":{"v":"User Interfaces","n":0.707},"1":{"v":"Here I outline how I see my users interacting with this application.\n\n## [[projects.local-recipe-server.user-interfaces.landing-page]]\nWhere a user first goes to be routed to either of the interfaces listed below\n\n## [[projects.local-recipe-server.user-interfaces.create-recipe]]\nThe interface that allows the user to create/edit recipes\n\n## [[projects.local-recipe-server.user-interfaces.view-recipe]]\nThe interface that allows the user to find and view the recipe.\n","n":0.144}}},{"i":17,"$":{"0":{"v":"View Recipe","n":0.707},"1":{"v":"\nThis interface is the pay off for all the hard work of parsing and checking text representations of recipes. A simple and easily readable representation of a recipe with no ads or unrelated content displayed.\n\nIt allows for searching recipes by name or ingredient, as well as filtering by ingredients.  The use cases supported are both finding a particular recipe the user is interested in as well as allowing the user to filter saved recipes by the ingredients they have/wish to consume.\n\nThe basic views of this interface are the list of recipes and the detail view of a specific recipe.\n\n## List View\nA simple list of recipe records saved by the user.  This view will support a search bar that searches against the recipe name (possible full text search?).  Filtering by ingredients will also be supported with an autocomplete field allowing the user to look up saved ingredients.\n\n## Detail View\nThis view will present the elements of the recipe (excluding the image records).  The styling will favor clear readability over decoration. Initial design is to attempt to present all ingredients in a single view, while displaying each step individually.  Simple controls to increment steps forward and back will facilitate navigation from step to step when cooking and interactions with either a touchscreen or a mouse/keyboard must be kept to a minimum.\n","n":0.068}}},{"i":18,"$":{"0":{"v":"Landing Page","n":0.707},"1":{"v":"\nWhile so basic it barely merits mentioning, it is important to not forget this element to provide a simple means of navigating on a mobile screen size.\n\nThis landing page will simply welcome the user and direct them to either the [[projects.local-recipe-server.user-interfaces.create-recipe]] or [[projects.local-recipe-server.user-interfaces.view-recipe]] interfaces.","n":0.151}}},{"i":19,"$":{"0":{"v":"Create Recipe","n":0.707},"1":{"v":"This interface allows the user to upload photos of recipes. The saving of the file to the application server file system will trigger the parsing service to extract text from the images and assign text to data models.\n\nDepending on the amount of time necessary to perform this action, the following step may occur within the same request/response loop or require a more asynchronous approach.  \n\nOnce the parsing service returns a representation of the extracted & assigned text, this should be presented to the user in a format that asupports modification of the text and data objects assigned as well as choose whether to save the recipe or cancel. Given the les-than-perfect images likely to be gathered, it seems like assuming occasional incorrect parsing of the images into text and/or incorrect data object assignment is a safe assumption to make.\n\nTherefore this interface will consist of two views.\n\n## Upload File(s)\nThis view needs to support the loading of multiple image files and making a POST request that transmits them to the server.\n\n## Edit/Save Results\nThis view will accept the structured text output from the parsing service and present the the user the ingredients, amounts, units, and steps as editable text.  At this point only the Recipe and Image records have been saved. If the use decides to cancel at this point, those records along with the associated files will be removed.  This allows the user to abandon the attempt if they decide they want to try better photos or the work of editing the resulting parsed text does not merit the saving of the recipe.\n\nIf the user selects the save option, this will create/assign the Ingredient, Step, and associative records to complete the recipe.  It will use the current representation of the text and structure in the user interface to ensure any changes made by the user in this view is reflected in the saved records.","n":0.057}}},{"i":20,"$":{"0":{"v":"Todo","n":1},"1":{"v":"A simple list of next steps in various parts of the application implementation\n\n## Front-End\n- [x] Add `ingredient` auto-complete field with `multiple, clearable, chips`\n- [x] Add `search` sync parameter to Recipes data table and trigger recipe list API requests on change.\n\n## API\n- [x] Convert Ingredient creation in the `confirm` action to `get_or_create` to avoid duplicating ingrdient records.\n- [x] Buid out `RecipeIngredient` Serializer.\n- [x] Add serialization of related `RecipeIngredient` records to the `Recipe` serializer and return complete recipe.\n- [x] Add `ingredients` search parameter to `Recipe` endpoint\n- [x] Modify `get_queryset` method for `Recipe` endpoint to accept ingredients as a filter\n- [x] Add an Ingredient Endpoint\n\n## Data Model\n- [x] Add a unique `name` constraint to the Ingredient data model\n\n\n## Services\n- [ ] Implement text recognition\n","n":0.091}}},{"i":21,"$":{"0":{"v":"Services","n":1},"1":{"v":"\nThe service layer in this application consists of the functions required to extract the text from images uploaded by the user and translate those into representations of the data structures be to created upon saving the recipe.\n\n## [[projects.local-recipe-server.services.text-parsing]]\nHow we go from images to text\n\n## [[projects.local-recipe-server.services.text-assigment]]\nWhat we do with the text once we've got it","n":0.136}}},{"i":22,"$":{"0":{"v":"Text Parsing","n":0.707},"1":{"v":"This service function consumes an array of image files or, more likely,filepaths and returns a single corpus of extracted text.  \n\n## Tech Choices\nThis is the area of this project I am least familiar with and the most likely to take some further research and experimentation to get right.  Based on initial cursory investigations, the Python package [PyTesseract](https://pypi.org/project/pytesseract/) appears to be a good place to start.\n\n## Output\nThis is perhaps a tricky thing to define.  While not all, some recipes include a preamble or story relating to how the recipe was arrived at or the cultural provenance.  While informative and engaging, these details are not pertinent to executing the recipe.  The intent of this application is to allow users to define those recipes they care about by saving them so it is assumed the user already knows why they should care about them.  \n\nHowever, it is not the job of this service function to make those decisions.  This service exists to extract text from image files and should not be burdened by addtional business logic.  The only functionality beyond extracting text from a single image file to be included here is the appending of text from successive images to a single corpus in the case of multiple images being assocaited with a single recipe.  The recognition of line breaks, event when between successive images, will also be key as these breaks will be necessary to inform the assignment function that consumes the extracted text.\n","n":0.064}}},{"i":23,"$":{"0":{"v":"Text Assigment","n":0.707},"1":{"v":"The purpose of this function will be to consume a corpus of text and, using imperitive rules, correctly assign text to the corresponding data model, thereby constructing a recipe.\n\n## Rules\nThe application of any sort of machine learning would require such a large set of data on which to train, as well as text recognition algorithms I am not familiar with, that I think it would be easier to simply write these rules myself.  Using line breaks, bullet or numbered formatting, and the presence/absence of numbers in the text will be used to classify distinct text elemets in the corpus this function accepts as it's sole parameter.\n\n## Output\nWhile this function will break out the text provided into the representations matching the data models present in this application, it does nto create records in the database.  These data model representations will be returned in JSON string formatting.  This will alow the editing of the text by users without excessive database write operations.\n","n":0.079}}},{"i":24,"$":{"0":{"v":"ML Tasks","n":0.707},"1":{"v":"After the images are uploaded but before we can create records in our database, we have two important ML tasks to perform.  These correspond to the [[projects.local-recipe-server.services.text-parsing]] and [[projects.local-recipe-server.services.text-assigment]] [[projects.local-recipe-server.services]] we have outlined elsewhere in this doc but I am documenting the ML particulars here.\n","n":0.149}}},{"i":25,"$":{"0":{"v":"OCR","n":1},"1":{"v":"\n* **Service:** [[projects.local-recipe-server.services.text-parsing]]\n* **Core Tech:** [PyTesseract](https://pypi.org/project/pytesseract/)\n\nAccurate Optical Character Recognition (OCR) is the core of the Text Parsing service.\n\nOne of the key challenges in text recognition and parsing with recipes is the use of columnar text layouts.\n\nAnother significant challenge is the translation of ingredient lists.  The main challenge here so far is proper recognition of fractions, although list format icons can add some extra difficulty.\n\n---\n\n## Columnar Text\nAn initial attempt to to handle proper word order when extracting text from multiple columns has been attempted following [this guide](https://nanonets.com/blog/ocr-with-tesseract/).  Results have been less than ideal but introducing some imperitive coding logic may prove useful as well as refining my image pre-processing steps to provide maximum contrast while maintaining clarity.\n\n### Next Steps\n* Conduct more research into the [Open CV](https://pypi.org/project/opencv-python/) functions for detecting contours in the image for better splitting of by columns.  This should improve segmentation for text processing.\n\n---\n\n## Ingredient Lists\nOne of the most difficult to interpret characters present in ingredient lists are fractions.  Often these are translated to `Y` or `%` characters.  Numbers (depending on the font used) can also provide a significant difficulty.\n\n### Next Steps\n* Review [this tutorial](https://pyimagesearch.com/2021/08/30/detecting-and-ocring-digits-with-tesseract-and-python/) about configuring PyTesseract for digit recognition.\n* Review [this SO Question](https://stackoverflow.com/questions/55994807/why-does-pytesseract-fail-to-recognise-digits-from-image-with-darker-background) regarding improved image preprocessing.\n* Review [this Github Issue](https://github.com/tesseract-ocr/tesseract/issues/2274) regarding applying new training data to improve PyTesseract model for fraction recognition.","n":0.068}}},{"i":26,"$":{"0":{"v":"NLP","n":1},"1":{"v":"* **Service:** [[projects.local-recipe-server.services.text-assigment]]\n* **Core Tech:** [NLTK](https://www.nltk.org/index.html)\n\nNatural Language Processing (NLP) is the segmentation, tokenization, and recognition of text into meaning.  This approach is what will be used to classify portions of the recipe into Ingredients and Steps.\n\n* **Ingredients:** Usually short sentence fragments that include ingredients and amounts.  In some cases they wil also include adverbs.\n\n* **Steps:** Longer, mostly full, sentences that are often directions of what to do and contain verbs pertaining to the ingredients.\n\nThe topic of NLP is quite broad and I think my next steps will need to include working through some general tutorials as well as some more focused on identifying the components outlined above.\n\n## Next Steps\n* Review [this RealPython tutorial](https://realpython.com/nltk-nlp-python/)\n* Review [this chapter](https://www.nltk.org/book_1ed/ch05.html) of the NLTK book on categorizing and tagging words.\n\n","n":0.089}}},{"i":27,"$":{"0":{"v":"Data Models","n":0.707},"1":{"v":"## Models\nThe basic data models defined to support this application are shown below:\n\n### Ingredient\n![[projects.local-recipe-server.data-models.ingredient]]\n\n### Recipe Ingredient\n![[projects.local-recipe-server.data-models.recipe-ingredient]]\n\n### Step\n![[projects.local-recipe-server.data-models.step]]\n\n### Image\n![[projects.local-recipe-server.data-models.image]]\n\n### Recipe\n![[projects.local-recipe-server.data-models.recipe]]\n\n## Thoughts\n### Steps\nI was/am not certain about breaking out steps into their own model or using a JSON field to store an array of parsed steps. However, I have some ideas for features I want to enable in the UI and I think that separate step data objects will facilitate this.  Perhaps that could have been implemented in the business logic layer or event in the presentation layer.  I may refactor this after some work on the user-interface.","n":0.102}}},{"i":28,"$":{"0":{"v":"Step","n":1},"1":{"v":"This model represents an individual step or set of instrutions for executing the recipe.  \n\n|Field|Type|Description|\n|----|----|----|\n|**ID**| BigInt| Primary Key|\n|**RecipeID**| BigInt| FK relating to Recipe|\n|**Order**| Int | Numeric represenation of the order of the step as it relates to the Recipe|\n|**Step**|Text| Recipe directions|\n","n":0.156}}},{"i":29,"$":{"0":{"v":"Recipe","n":1},"1":{"v":"Putting it all together.\n\n|Field|Type|Description|\n|----|----|----|\n|**ID**| BigInt| Primary Key|\n|**Name**| Text | The name for this recipe|\n\n","n":0.267}}},{"i":30,"$":{"0":{"v":"Recipe Step","n":0.707},"1":{"v":"This is the associate \n","n":0.447}}},{"i":31,"$":{"0":{"v":"Recipe Ingredient","n":0.707},"1":{"v":"\nThis model is the associative entity that relates ingredients to recipes. It stores data that are unique to this relationship, i.e. quantity and unit.\n\n|Field|Type|Description|\n|----|----|----|\n|**ID**| BigInt| Primary Key\n|**RecipeID**|BigInt| FK relating to Recipe|\n|**IngredientID**|BigInt| FK relating to Ingredient|\n|**Quantity**|Int| Quantity of the ingredient in the recipe|\n|**Unit**|Text| The unit the quantity represents (e.g. cup, pinc, teaspoon)|\n","n":0.14}}},{"i":32,"$":{"0":{"v":"Ingredient","n":1},"1":{"v":"This model represents an individual ingredient used in a recipe.\n\n\n|Field|Type|Description|\n|----|----|----|\n|**ID**| BigInt| Primary Key\n|**Name**|Text| Name of the ingredient|\n\n","n":0.243}}},{"i":33,"$":{"0":{"v":"Image","n":1},"1":{"v":"\nSince it may take more than one image to contain all the text in a readable format, it is nessary to allow more than one image to be associated with a recipe.\n\n|Field|Type|Description|\n|----|----|----|\n|**ID**| BigInt| Primary Key|\n|**RecipeID**| BigInt | FK relating to Recipe|\n|**ImageFile**| FileField| Link to image file in filesystem|\n","n":0.144}}},{"i":34,"$":{"0":{"v":"API","n":1},"1":{"v":"This section defines how the client will interact with what resources in our application.  Since we offer some complex features (on the Create Recipe side), this will be more than just a simple CRUD REST API.\n\n## Create Recipe Endpoints\n\n* **POST** `/recipes` - Initial recipe creation endpoint, this will initiate the text recognition and parsing processes.\n    * **Parameters:** \n        * `images` - An array of image files representing a single recipe\n    * **Returns:**\n        * `recipe_json` - a JSON representation of the parsed recipe\n\n* **POST** `/recipes/confirm` - User confirmation/correction of parsed recipe text and organization\n    * **Parameters:**\n        * `recipe_json` - JSON representation of user confirmed/corrected recipe\n    * **Returns:**\n        * `recipe_id` - The ID identifying the recipe record created in the DB. This can also be used in a redirect to the detail view of the created recipe.\n\n---\n\n## Read Recipe Endpoints\n\n* **GET** `/recipes` - A list endpoint for viewing existing recipe records\n    * **Parameters:**\n        * `ingredients` - (Optional) filters recipes returned by relationship to the ingredient(s).\n    * **Returns:**\n        * `recipes` - A paginated list of recipes.\n* **GET** `/recipes/:id` - A detail endpoint for viewing an individual recipe.\n    * **Parameters:** - `None`\n    * **Returns:**\n        * `recipe` - A single recipe record, including ingredients and steps.\n\n* **GET** `/ingredients` - A list endpoint for ingredient records\n    * **Parameters:** - `None`\n    * **Returns:**\n        * `ingredients` - An unpaginated list of ingredients\n\n* **GET** `/recipes/:id/ingredients` - A list endpoint for ingredients related to a single recipe.\n    * **Parameters:** - `None`\n    * **Returns:**\n        * `ingredients` - A list of ingredients for a specific recipe\n\n* **GET** `/recipes/:id/steps` - A list endpoint for steps related to a single recipe.\n    * **Parameters:** - `None`\n    * **Returns:**\n        * `steps` - A list of steps to execute the recipe","n":0.059}}},{"i":35,"$":{"0":{"v":"Serializers","n":1},"1":{"v":"Because our recipes consist of multiple data models but we want to to present them as whole documents we need to coordinate the serialization.\n\n## Recipe-Ingredient Serializer\nWhen we are serializing the ingredients for a recipe, most of what we want is actually in the relationship entity.  This serializer should return the following fields as a JSON object.\n* `name` - [[projects.local-recipe-server.data-models.ingredient]] data model\n* `quantity` - [[projects.local-recipe-server.data-models.recipe-ingredient]] data model\n* `unit` - [[projects.local-recipe-server.data-models.recipe-ingredient]] data model\n\n## Recipe Serializer\nThis serializer returns the data object that the List/Detail components of our API will interact with.  It will return the following fields.\n* `title` - [[projects.local-recipe-server.data-models.recipe]] data model\n* `steps` - [[projects.local-recipe-server.data-models.recipe]] data model\n* `recipe-ingredients` - An array of serialized [[projects.local-recipe-server.data-models.recipe-ingredient]] data models\n","n":0.094}}},{"i":36,"$":{"0":{"v":"Android App Design","n":0.577},"1":{"v":"_This section is intended to capture design decisions for a potential Android application as part of the Recipes Server_\n\n---\n\nOne of the important choices to make is how much data to store locally, if at all.  The book I am working through to build Android apps is making use of the [Room API](https://developer.android.com/jetpack/androidx/releases/room) to perform CRUD operations with an SQLite DB saved locally on the Android device's file system.  This makes me start to question an entirely API reliant design.\n\n## Data Storage question\nMy question for this app is whether it would make more sense to use the local server just for performing the [[projects.local-recipe-server.services.text-parsing]] and [[projects.local-recipe-server.services.text-assigment]] functionality and store the final recipe record \nlocally or whether I should rely on the REST API entirely.  While this would simplify application design it would mean the apps functionality would entirely depend on the health and availability of a Raspberry Pi runningon my local network\n\n### Initial decision\n* Proceed with entirely REST API driven approach \n* Be prepared to transition to more data storage in the Android device itself as I become more familiar with caching and synchronizing local data with the canonical server database.","n":0.073}}},{"i":37,"$":{"0":{"v":"Automated Analysis","n":0.707},"1":{"v":"\n## Motivation\nAfter a break in early spring storms I noticed a vivid bloom of phytoplankton along the Monterey peninsula coast.  When I first saw it the stratification of the water masses was pretty extreme. I realized that the timing appeared to line up with the delivery of previously upwelled water that I described in my M.Sc. thesis.  This led me to wish I could visualize the timing and geographic spread of this event to see if it really lined up with a relaxation after a fully established upwelling circulation pattern had been established.  Given that almost all the data I used was easily publicly accessible, this is something that _should_ be a candidate for automation.  With that I started thinking how I would go about it.\n\n## Purpose\nThis project should produce a a codebase that permits the automating the collection, visualization, and analysis of oceanographic data to identify upwelling patterns.  To do this it will examine wind patterns, surface currents, and sa surface temperature in the Nonterey Bay area.  \n\n## Data Sources\n* Wind\n    * NDBC 46042 (https://www.ndbc.noaa.gov/station_page.php?station=46042)\n* Surface Currents (HF Radar) - 2KM resolution\n    * NOAA THREDS (https://dods.ndbc.noaa.gov/thredds/hfradar.html?dataset=hfradar_uswc_2km)\n* Sea Surface Temperature\n    * NDBC 46042 (https://www.ndbc.noaa.gov/station_page.php?station=46042)\n    * NDBC 46240 (https://www.ndbc.noaa.gov/station_page.php?station=46240)\n    * NASA PODAAC (https://podaac.jpl.nasa.gov/dataaccess)\n\n## Outstanding Questions\n* **File format** - The different data sources do not all use consistent formats.  Determining how to access each file type and get the data loaded into memory for analysis is something that will need to be addressed.\n* **Graph types** - MatLab (the software used for my thesis analysis) had a lot of built in scientific graphing functionality.  Finding the correct libraries and approaches to building something similar in Python - NO MORE LICENSED SOFTWARE!\n\n## Design Ideas\n\n### API Service\nAll the data used for this project is accessible via some form of HTTP or FTP request.  Thus it makes sense to design an API service that, based on resource being retrieved, makes the appropriate API request and returns the data.\n\n### Data Service\nI'm not sure what the correct design pattern name is here but basically this service should abstract the loading of data into memory based on the file type.  It should expose a single function that returns the data from the API service in a format suitable for analysis and visualization.  The business logic of this service is to handle accessing the file based on the type (e.g. NetCDF, HFD, CSV) as well as retrieving the data of interest.  The level of nesting a metadata will vary between data sources and this should be handled here too.\n\n### Computed Data Service\n\nThe purpose of this service is to calculate those vectors that are necessary for the eventual analysis but not directly measured and captured in the data sources themselves.  The clearest example is calculating wind stress from wind speed and direction as well as decomposing wind stress into alongshore and cross-shore vectors.  This service should also handle any transformations necessary so that the data product returned is ready to visualization and analysis.\n\n### Visualization Service\n\nThis service does the work of generating all the desired data visualizations. \n\n### Statistical Service\n\nThis should perform all the statistical analyses (e.g. correlation, timing) required to identify whether or not upwelling events and reversals have occurred during the time period of interest.","n":0.043}}}]}
